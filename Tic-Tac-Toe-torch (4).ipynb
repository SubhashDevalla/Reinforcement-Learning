{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca196dea-d779-4f6a-8490-ee662741eb27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Any\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from collections import deque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633acaa2-8246-4d2f-b8b7-0eeaaeb2d8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(random.randint(0, 1000))\n",
    "mean = 0.0\n",
    "stddev = 0.05\n",
    "def random_init_fn():\n",
    "    return nn.init.normal_(torch.empty(1), mean=0.0, std=0.05).item()\n",
    "random_init = torch.nn.init.normal_(torch.empty(1), mean=mean, std=stddev).item()\n",
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, grid_size=(3, 3)):\n",
    "        super(TicTacToe, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(2, grid_size[0], grid_size[1]), dtype=np.uint8)\n",
    "        self.steps=0\n",
    "        self.agent_sym=self.get_agent()\n",
    "        self.opponent_sym=\"X\" if self.agent_sym==\"O\" else \"O\"\n",
    "        self.board=self.build_board()\n",
    "        self.placed=[]\n",
    "#         self.root = tk.Tk()\n",
    "#         self.root.title(\"TicTacToe\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.steps=0\n",
    "        self.agent_sym=self.get_agent()\n",
    "        self.opponent_sym=\"X\" if self.agent_sym==\"O\" else \"O\"\n",
    "        self.board=self.build_board()\n",
    "        self.placed=[]\n",
    "        return self._get_observation()\n",
    "\n",
    "    def get_agent(self):\n",
    "        return random.choice(['X','O'])\n",
    "\n",
    "    def build_board(self):\n",
    "        return np.zeros([3,3],dtype='str')\n",
    "\n",
    "    def encode_board(self):\n",
    "        # enboard=np.zeros([3,3])\n",
    "        # for i in range(3):\n",
    "        #     for j in range(3):\n",
    "        #         if(self.board[i,j]==\"X\"):\n",
    "        #             enboard[i,j]=1\n",
    "        #         elif(self.board[i,j]==\"O\"):\n",
    "        #             enboard[i,j]=-1\n",
    "        #         else:\n",
    "        #             enboard[i,j]=0\n",
    "        # if self.turn() != \"X\":\n",
    "        #     enboard = enboard * -1\n",
    "\n",
    "        current_player = 0 if self.turn() == \"X\" else 1\n",
    "        flag_board = np.zeros((2,3,3))\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i,j] == self.agent_sym:\n",
    "                    flag_board[0,i,j] = 1\n",
    "                elif self.board[i,j] == self.opponent_sym:\n",
    "                    flag_board[1,i,j] = 1\n",
    "\n",
    "        if current_player != 0:\n",
    "            flag_board_first = flag_board[0,:,:].copy()\n",
    "            flag_board[0,:,:] = flag_board[1,:,:]\n",
    "            flag_board[1,:,:] = flag_board_first\n",
    "        debuged = flag_board.swapaxes(0,-1)\n",
    "        return flag_board\n",
    "\n",
    "    def GetPosMoves(self,board):\n",
    "        pos_moves=[]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "              if(board[i,j]==\"\" or board[i,j]==\"0\"):\n",
    "                pos_moves.append(i*3 + j)\n",
    "        return pos_moves\n",
    "\n",
    "    def is_game_over(self,moves):\n",
    "        winner=self.get_winner()\n",
    "        # print(\"Winner:\",winner,\"Agent:\",self.agent_sym,\"Opponent:\",self.opponent_sym)\n",
    "        if(winner==self.agent_sym):\n",
    "            return 1,1\n",
    "        elif(winner==self.opponent_sym):\n",
    "            return 2,-1\n",
    "        else:\n",
    "            if(self.steps>8 and len(moves)==0):\n",
    "                # print(self.steps)\n",
    "                return -1,0.5\n",
    "            else:\n",
    "                return 0,0\n",
    "\n",
    "#     def get_reward(self,move):\n",
    "#         i,j=move\n",
    "#         reward=0\n",
    "#         temp1=copy.deepcopy(self.board)\n",
    "#         turn1 = \"X\" if self.turn()==\"X\" else \"O\"\n",
    "#         turn2 = \"O\" if turn1==\"X\" else \"X\"\n",
    "#         temp1[i,j]=turn1\n",
    "#         winner=self.get_winner(temp1)\n",
    "#         if(winner==turn1):\n",
    "#             reward-=1\n",
    "#             # print(\"Turn1\")\n",
    "#         temp1[i,j]=turn2\n",
    "#         if(self.get_winner(temp1)==turn2):\n",
    "#             reward+=0.2\n",
    "#         return reward\n",
    "        \n",
    "\n",
    "    def get_winner(self,board):\n",
    "        count=0\n",
    "        for t in range(3):\n",
    "            if(board[t,0]==board[t,1] and board[t,1]==board[t,2] and board[t,0]!=\"\"):\n",
    "                return board[t,0]\n",
    "            if(board[0,t]==board[1,t] and board[1,t]==board[2,t] and board[0,t]!=\"\"):\n",
    "\n",
    "                return board[0,t]\n",
    "\n",
    "        if(board[1,1]==board[0,0] and board[1,1]==board[2,2] and board[1,1]!=\"\"):\n",
    "            return board[0,0]\n",
    "\n",
    "        elif(board[0,2]==board[1,1] and board[1,1]==board[2,0] and board[1,1]!=\"\"):\n",
    "            return board[1,1]\n",
    "        return None\n",
    "\n",
    "    def turn(self):\n",
    "        if(self.steps%2==0):\n",
    "            return \"X\"\n",
    "        else:\n",
    "            return \"O\"\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.is_valid(action) == False:\n",
    "            self.steps+=1\n",
    "            return self.encode_board() , 1.1 , True\n",
    "        action = self.decode_move(action)\n",
    "        # reward = -self.get_reward(action)\n",
    "        i, j = action\n",
    "        self.board[i,j]= self.turn()\n",
    "        self.steps+=1\n",
    "        reward=0\n",
    "        winner=self.get_winner(self.board)\n",
    "        done = True if winner is not None or self.steps>8 else False\n",
    "        if done:\n",
    "            reward = 0.25 if winner is None else -1\n",
    "        return self.encode_board() , reward , done\n",
    "\n",
    "    def render(self):\n",
    "        board=copy.deepcopy(self.board)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if(board[i,j]==\"\"):\n",
    "                    board[i,j]=\"0\"\n",
    "        for row in board:\n",
    "            formatted_row = [val for val in row]\n",
    "\n",
    "            print(\" | \".join(formatted_row))\n",
    "            print(\"-\" * 9)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return self.encode_board()\n",
    "\n",
    "#     def custom_board(self,str1):\n",
    "#         board=np.zeros([6,7],dtpye=\"str\")\n",
    "#         for i in range(7):\n",
    "#             for j in range(int(str[i])-1,-1,-1):\n",
    "#                 board[i]\n",
    "#         board=board.reshape(3,3)\n",
    "#         return board\n",
    "\n",
    "    def encode_move(self,move):\n",
    "        i,j=move\n",
    "        return i*3 + j\n",
    "\n",
    "    def decode_move(self,move):\n",
    "        return (move//3,move%3)\n",
    "\n",
    "    # def render1(self):                                   #used to render the board into tkinter gui\n",
    "    #     for i in range(3):\n",
    "    #         for j in range(3):\n",
    "    #             piece = self.board[i][j]\n",
    "    #             color = \"#E3C16F\" if (i + j) % 2 == 0 else \"#B88B4A\"\n",
    "    #             if(piece==\"\"):\n",
    "    #                 piece=\"    \"\n",
    "    #             self.label = tk.Label(self.root, text=\"    \", font=(\"Helvetica\", 21),bg=color)\n",
    "    #             self.label.grid(row=i, column=j)\n",
    "    #             self.label = tk.Label(self.root, text=piece, font=(\"Helvetica\", 21),bg=color)\n",
    "    #             self.label.grid(row=i, column=j)\n",
    "    #     self.root.update()\n",
    "\n",
    "    def is_valid(self,move):\n",
    "        move=self.decode_move(move)\n",
    "        i,j=move\n",
    "        if(self.board[i,j]==\"\"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def close(self):\n",
    "        self.root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b58201-0d11-4a9c-8a58-f85ebcf827bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate,env,actor):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self.build_model()\n",
    "        self.model.apply(self.init_weights)  # Apply random weight initialization\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        # self.model.compile(optimizer=self.optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def build_model(self):\n",
    "        model=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=state_dim[0], out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * state_dim[1] * state_dim[2], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "            init.normal_(layer.weight, mean=0.0, std=0.05)\n",
    "            init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    \n",
    "    def get_actions_prob(self, states):\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            action_probs_tensor = self.model(states_tensor)\n",
    "        action_probs = action_probs_tensor.numpy()\n",
    "        return action_probs\n",
    "\n",
    "    def train(self, states, actions, advantages,masks):\n",
    "        masks_tensor = torch.tensor(masks,dtype=torch.float32)\n",
    "        advantages_tensor:torch.Tensor = torch.tensor(advantages,dtype=torch.float32)\n",
    "        advantages_tensor = torch.squeeze(advantages_tensor)\n",
    "        actions_tensor = torch.tensor(actions,dtype=torch.float32)\n",
    "        states_tensor = torch.tensor(states,dtype=torch.float32)\n",
    "        action_probs = self.model(states_tensor)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        entropy_loss = dist.entropy().mean()\n",
    "        log_probs = dist.log_prob(actions_tensor)\n",
    "        actor_loss = -log_probs * advantages_tensor\n",
    "        loss = actor_loss.mean() - 0.50 * entropy_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, learning_rate,critic):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.build_model()\n",
    "        self.model.apply(self.init_weights)  # Apply random weight initialization\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        # self.model.compile(optimizer=self.optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    def build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=state_dim[0], out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * state_dim[1] * state_dim[2], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "            init.normal_(layer.weight, mean=0.0, std=0.05)\n",
    "            init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def get_values(self, states):\n",
    "        states_tensor = torch.tensor(states,dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            values_tensor = self.model(states_tensor)\n",
    "        values_tensor = torch.squeeze(values_tensor)\n",
    "        values = values_tensor.numpy()\n",
    "        return values\n",
    "\n",
    "    def train(self, states, discounted_rewards):\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "        discounted_rewards_tensor = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "        values = self.model(states_tensor)\n",
    "        loss = torch.mean((discounted_rewards_tensor - values) ** 2)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        del states_tensor, discounted_rewards_tensor, values, loss\n",
    "\n",
    "# A2C algorithm\n",
    "class A2C:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate_actor, learning_rate_critic,env,actor,critic):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.actor = Actor(state_dim, action_dim, learning_rate_actor,env,actor)\n",
    "        self.critic = Critic(state_dim, learning_rate_critic,critic)\n",
    "\n",
    "    def select_action(self, state,legal_actions,temp=0):\n",
    "        assert len(legal_actions) > 0\n",
    "        action_probs = self.actor.get_actions_prob(state)\n",
    "        debugged_state= np.swapaxes(state,-3,-1)\n",
    "        masks= np.zeros((self.action_dim,),dtype=np.float32)\n",
    "        masks[legal_actions] = 1\n",
    "        probs = action_probs[0]\n",
    "        if(temp==1):\n",
    "            print(probs)\n",
    "            probs = probs * masks\n",
    "            probs = probs / np.sum(probs,axis=-1)\n",
    "            action=np.argmax(probs)\n",
    "        else:\n",
    "            # print(probs)\n",
    "            action = np.random.choice(self.action_dim,p=probs)\n",
    "        return action\n",
    "\n",
    "    def train(self,memory_batch):\n",
    "        states_batch=[]\n",
    "        action_batch=[]\n",
    "        returns_batch=[]\n",
    "        masks=[]\n",
    "        # print(memory_batch)\n",
    "        for i in range(len(memory_batch)):\n",
    "            states_batch.append(memory_batch[i][0])\n",
    "            action_batch.append(memory_batch[i][1])\n",
    "            returns_batch.append(memory_batch[i][2])\n",
    "            masks.append(memory_batch[i][3])\n",
    "        states_batch=np.array(states_batch)\n",
    "        action_batch=np.array(action_batch)\n",
    "        returns_batch=np.array(returns_batch)\n",
    "        masks=np.array(masks)\n",
    "        values = self.critic.get_values(states_batch)\n",
    "        advantages = returns_batch - values\n",
    "        self.actor.train(states_batch, action_batch, advantages,masks)\n",
    "        self.critic.train(states_batch, returns_batch)\n",
    "        return advantages\n",
    "        del values,next_values,td_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c21e24-7d93-4c2c-a973-aebdeca020ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_vs_random_player(a2c:A2C,n_games:int):\n",
    "    env  = TicTacToe()\n",
    "    endingscore = 0\n",
    "    for i in range(n_games):\n",
    "        agent_player = random.randint(0,1) %2\n",
    "        random_player = 1- agent_player\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        current_player = 0\n",
    "        while not done:\n",
    "            print(env.board)\n",
    "            moves = env.GetPosMoves(env.board)\n",
    "            if current_player == agent_player:\n",
    "                action = a2c.select_action([state],moves,1)\n",
    "            else:\n",
    "                action = int(input()) # for playing against a human\n",
    "                # action = np.random.choice(moves)\n",
    "\n",
    "            next_state, reward , done = env.step(action)\n",
    "            current_player = 1-current_player\n",
    "            state= next_state\n",
    "\n",
    "        if current_player != agent_player:\n",
    "            if(reward<=-0.25):\n",
    "                endingscore -= -1\n",
    "\n",
    "    agentwins = endingscore\n",
    "    return agentwins / n_games * 100\n",
    "\n",
    "\n",
    "def calculate_returns(rewards,dones,last_value):\n",
    "    rewards = np.array(rewards)\n",
    "    returns = np.zeros_like(rewards)\n",
    "\n",
    "    gamma = 1.0\n",
    "    T = len(rewards)\n",
    "    flag = 0\n",
    "    next_value = last_value\n",
    "    for t in reversed(range(T)):\n",
    "        opponent_reward = rewards[t]\n",
    "        current_reward = -opponent_reward\n",
    "        if dones[t] and rewards[t]==1:\n",
    "            next_value = 0\n",
    "            flag = 1\n",
    "            temp=int(t)\n",
    "        elif dones[t] and rewards[t]!=-1:\n",
    "            next_value = 0\n",
    "            flag = 0\n",
    "        current_value = current_reward - gamma * next_value\n",
    "        if(flag and temp==t):\n",
    "            returns[t] = current_value\n",
    "            next_value = current_value\n",
    "            temp=-2\n",
    "        elif(flag and temp!=t):\n",
    "            returns[t] = 0.005\n",
    "            next_value = current_value\n",
    "        else:\n",
    "            returns[t] = current_value\n",
    "            next_value = current_value\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3e57f4-6a23-4865-88f8-f9d0d767f5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 745\n",
      "steps: 1000 Win ratio against random player 50.00\n",
      "count: 285\n",
      "steps: 2000 Win ratio against random player 70.00\n",
      "count: 258\n",
      "steps: 3000 Win ratio against random player 52.00\n",
      "count: 341\n",
      "steps: 4000 Win ratio against random player 54.00\n",
      "count: 334\n",
      "steps: 5000 Win ratio against random player 64.00\n",
      "count: 264\n",
      "steps: 6000 Win ratio against random player 64.00\n",
      "count: 129\n",
      "steps: 7000 Win ratio against random player 68.00\n",
      "count: 125\n",
      "steps: 8000 Win ratio against random player 58.00\n",
      "count: 136\n",
      "steps: 9000 Win ratio against random player 68.00\n",
      "count: 182\n",
      "steps: 10000 Win ratio against random player 60.00\n",
      "count: 165\n",
      "steps: 11000 Win ratio against random player 56.00\n",
      "count: 191\n",
      "steps: 12000 Win ratio against random player 70.00\n",
      "count: 347\n",
      "steps: 13000 Win ratio against random player 62.00\n",
      "count: 276\n",
      "steps: 14000 Win ratio against random player 50.00\n",
      "count: 99\n",
      "steps: 15000 Win ratio against random player 66.00\n",
      "count: 5\n",
      "steps: 16000 Win ratio against random player 60.00\n",
      "count: 4\n",
      "steps: 17000 Win ratio against random player 64.00\n",
      "count: 1\n",
      "steps: 18000 Win ratio against random player 70.00\n",
      "count: 0\n",
      "steps: 19000 Win ratio against random player 56.00\n",
      "count: 1\n",
      "steps: 20000 Win ratio against random player 62.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_fn = lambda : TicTacToe()\n",
    "    env = env_fn()\n",
    "    # moded\n",
    "    state_dim= env.observation_space.shape\n",
    "    action_dim=9\n",
    "    learning_rate_actor = 0.0001\n",
    "    learning_rate_critic = 0.0001\n",
    "    a2c = A2C(state_dim, action_dim, learning_rate_actor, learning_rate_critic,env,actor=None,critic=None)\n",
    "    count1=0\n",
    "    horizon = 7\n",
    "    max_steps = 20000\n",
    "    batch_size = 128\n",
    "    total_steps=0\n",
    "    state=env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    all_masks = []\n",
    "    memory=[]\n",
    "    while total_steps < max_steps:\n",
    "        for step in range(horizon):\n",
    "            moves = env.GetPosMoves(env.board)\n",
    "            masks= np.zeros((action_dim,),dtype=np.float32)\n",
    "            masks[moves] = 1\n",
    "            # print(env.steps,moves)\n",
    "            temp1= a2c.select_action([state],moves)\n",
    "            is_legal_action=env.is_valid(temp1)\n",
    "            if(is_legal_action==False):\n",
    "                count1+=1\n",
    "            temp=int(temp1)\n",
    "            action = temp\n",
    "            next_state , reward , done = env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            all_masks.append(masks)\n",
    "            if done:\n",
    "                next_state = env.reset()\n",
    "            state = next_state\n",
    "        total_steps+=1\n",
    "        # print(total_steps)\n",
    "        last_state=state\n",
    "        # print(state)\n",
    "        last_value = a2c.critic.get_values([state])\n",
    "        # print(last_value)\n",
    "        \n",
    "        returns = calculate_returns(rewards,dones,last_value)\n",
    "        for i in range(len(states)):\n",
    "            memory.append([states[i],actions[i],returns[i],all_masks[i]])\n",
    "        if(len(states)<batch_size):\n",
    "            pass\n",
    "        else:\n",
    "            memory=random.sample(memory,batch_size)\n",
    "        \n",
    "        ad=a2c.train(\n",
    "                memory,\n",
    "            )\n",
    "            \n",
    "        if(total_steps % 1000==999):\n",
    "            print(\"count:\",count1)\n",
    "            print(\"steps:\",total_steps+1,end=\" \")\n",
    "            win_ratio = test_vs_random_player(a2c , 50)\n",
    "            print(f\"Win ratio against random player {win_ratio:0.2f}\")\n",
    "            count1=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda2237-e0b1-44a1-8659-1b9ae7271f36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '' '']\n",
      " ['' '' '']\n",
      " ['' '' '']]\n",
      "[7.1423888e-11 1.7172843e-01 2.4276604e-01 1.2158324e-07 1.6845330e-07\n",
      " 3.4384843e-08 1.7631900e-01 1.3036069e-07 4.0918607e-01]\n",
      "[['' '' '']\n",
      " ['' '' '']\n",
      " ['' '' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' '' '']\n",
      " ['' '' '']\n",
      " ['' '' 'X']]\n",
      "[1.6170984e-42 5.7881302e-08 8.6037836e-08 4.5335641e-12 7.9697637e-11\n",
      " 2.1346855e-08 9.9999976e-01 7.1927393e-22 2.7418259e-24]\n",
      "[['O' '' '']\n",
      " ['' '' '']\n",
      " ['X' '' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' '' '']\n",
      " ['' '' '']\n",
      " ['X' 'O' 'X']]\n",
      "[9.8339951e-29 2.7386171e-09 3.9277919e-09 9.9926585e-01 6.8576005e-04\n",
      " 4.8339494e-05 3.1429248e-32 0.0000000e+00 2.7517724e-26]\n",
      "[['O' '' '']\n",
      " ['X' '' '']\n",
      " ['X' 'O' 'X']]\n"
     ]
    }
   ],
   "source": [
    "win_ratio = test_vs_random_player(a2c , 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
