{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca196dea-d779-4f6a-8490-ee662741eb27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Any\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from collections import deque \n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_,clip_grad_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633acaa2-8246-4d2f-b8b7-0eeaaeb2d8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(random.randint(0, 1000))\n",
    "mean = 0.0\n",
    "stddev = 0.05\n",
    "def random_init_fn():\n",
    "    return nn.init.normal_(torch.empty(1), mean=0.0, std=0.05).item()\n",
    "random_init = torch.nn.init.normal_(torch.empty(1), mean=mean, std=stddev).item()\n",
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, grid_size=(3, 3)):\n",
    "        super(TicTacToe, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(2, grid_size[0], grid_size[1]), dtype=np.uint8)\n",
    "        self.steps=0\n",
    "        self.agent_sym=self.get_agent()\n",
    "        self.opponent_sym=\"X\" if self.agent_sym==\"O\" else \"O\"\n",
    "        self.board=self.build_board()\n",
    "        self.placed=[]\n",
    "#         self.root = tk.Tk()\n",
    "#         self.root.title(\"TicTacToe\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.steps=0\n",
    "        self.agent_sym=self.get_agent()\n",
    "        self.opponent_sym=\"X\" if self.agent_sym==\"O\" else \"O\"\n",
    "        self.board=self.build_board()\n",
    "        self.placed=[]\n",
    "        return self._get_observation()\n",
    "\n",
    "    def get_agent(self):\n",
    "        return random.choice(['X','O'])\n",
    "\n",
    "    def build_board(self):\n",
    "        return np.zeros([3,3],dtype='str')\n",
    "\n",
    "    def encode_board(self):\n",
    "        # enboard=np.zeros([3,3])\n",
    "        # for i in range(3):\n",
    "        #     for j in range(3):\n",
    "        #         if(self.board[i,j]==\"X\"):\n",
    "        #             enboard[i,j]=1\n",
    "        #         elif(self.board[i,j]==\"O\"):\n",
    "        #             enboard[i,j]=-1\n",
    "        #         else:\n",
    "        #             enboard[i,j]=0\n",
    "        # if self.turn() != \"X\":\n",
    "        #     enboard = enboard * -1\n",
    "\n",
    "        current_player = 0 if self.turn() == \"X\" else 1\n",
    "        flag_board = np.zeros((2,3,3))\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i,j] == self.agent_sym:\n",
    "                    flag_board[0,i,j] = 1\n",
    "                elif self.board[i,j] == self.opponent_sym:\n",
    "                    flag_board[1,i,j] = 1\n",
    "\n",
    "        if current_player != 0:\n",
    "            flag_board_first = flag_board[0,:,:].copy()\n",
    "            flag_board[0,:,:] = flag_board[1,:,:]\n",
    "            flag_board[1,:,:] = flag_board_first\n",
    "        debuged = flag_board.swapaxes(0,-1)\n",
    "        return flag_board\n",
    "\n",
    "    def GetPosMoves(self,board):\n",
    "        pos_moves=[]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "              if(board[i,j]==\"\" or board[i,j]==\"0\"):\n",
    "                pos_moves.append(i*3 + j)\n",
    "        return pos_moves\n",
    "\n",
    "    def is_game_over(self,moves):\n",
    "        winner=self.get_winner()\n",
    "        # print(\"Winner:\",winner,\"Agent:\",self.agent_sym,\"Opponent:\",self.opponent_sym)\n",
    "        if(winner==self.agent_sym):\n",
    "            return 1,1\n",
    "        elif(winner==self.opponent_sym):\n",
    "            return 2,-1\n",
    "        else:\n",
    "            if(self.steps>8 and len(moves)==0):\n",
    "                # print(self.steps)\n",
    "                return -1,0.5\n",
    "            else:\n",
    "                return 0,0\n",
    "\n",
    "#     def get_reward(self,move):\n",
    "#         i,j=move\n",
    "#         reward=0\n",
    "#         temp1=copy.deepcopy(self.board)\n",
    "#         turn1 = \"X\" if self.turn()==\"X\" else \"O\"\n",
    "#         turn2 = \"O\" if turn1==\"X\" else \"X\"\n",
    "#         temp1[i,j]=turn1\n",
    "#         winner=self.get_winner(temp1)\n",
    "#         if(winner==turn1):\n",
    "#             reward-=1\n",
    "#             # print(\"Turn1\")\n",
    "#         temp1[i,j]=turn2\n",
    "#         if(self.get_winner(temp1)==turn2):\n",
    "#             reward+=0.2\n",
    "#         return reward\n",
    "        \n",
    "\n",
    "    def get_winner(self,board):\n",
    "        count=0\n",
    "        for t in range(3):\n",
    "            if(board[t,0]==board[t,1] and board[t,1]==board[t,2] and board[t,0]!=\"\"):\n",
    "                return board[t,0]\n",
    "            if(board[0,t]==board[1,t] and board[1,t]==board[2,t] and board[0,t]!=\"\"):\n",
    "\n",
    "                return board[0,t]\n",
    "\n",
    "        if(board[1,1]==board[0,0] and board[1,1]==board[2,2] and board[1,1]!=\"\"):\n",
    "            return board[0,0]\n",
    "\n",
    "        elif(board[0,2]==board[1,1] and board[1,1]==board[2,0] and board[1,1]!=\"\"):\n",
    "            return board[1,1]\n",
    "        return None\n",
    "\n",
    "    def turn(self):\n",
    "        if(self.steps%2==0):\n",
    "            return \"X\"\n",
    "        else:\n",
    "            return \"O\"\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.is_valid(action) == False:\n",
    "            self.steps+=1\n",
    "            return self.encode_board() , 1.1 , True\n",
    "        action = self.decode_move(action)\n",
    "        # reward = -self.get_reward(action)\n",
    "        i, j = action\n",
    "        self.board[i,j]= self.turn()\n",
    "        self.steps+=1\n",
    "        reward=0\n",
    "        winner=self.get_winner(self.board)\n",
    "        done = True if winner is not None or self.steps>8 else False\n",
    "        if done:\n",
    "            reward = 0.25 if winner is None else -1\n",
    "        return self.encode_board() , reward , done\n",
    "\n",
    "    def render(self):\n",
    "        board=copy.deepcopy(self.board)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if(board[i,j]==\"\"):\n",
    "                    board[i,j]=\"0\"\n",
    "        for row in board:\n",
    "            formatted_row = [val for val in row]\n",
    "\n",
    "            print(\" | \".join(formatted_row))\n",
    "            print(\"-\" * 9)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return self.encode_board()\n",
    "\n",
    "#     def custom_board(self,str1):\n",
    "#         board=np.zeros([6,7],dtpye=\"str\")\n",
    "#         for i in range(7):\n",
    "#             for j in range(int(str[i])-1,-1,-1):\n",
    "#                 board[i]\n",
    "#         board=board.reshape(3,3)\n",
    "#         return board\n",
    "\n",
    "    def encode_move(self,move):\n",
    "        i,j=move\n",
    "        return i*3 + j\n",
    "\n",
    "    def decode_move(self,move):\n",
    "        return (move//3,move%3)\n",
    "\n",
    "    # def render1(self):                                   #used to render the board into tkinter gui\n",
    "    #     for i in range(3):\n",
    "    #         for j in range(3):\n",
    "    #             piece = self.board[i][j]\n",
    "    #             color = \"#E3C16F\" if (i + j) % 2 == 0 else \"#B88B4A\"\n",
    "    #             if(piece==\"\"):\n",
    "    #                 piece=\"    \"\n",
    "    #             self.label = tk.Label(self.root, text=\"    \", font=(\"Helvetica\", 21),bg=color)\n",
    "    #             self.label.grid(row=i, column=j)\n",
    "    #             self.label = tk.Label(self.root, text=piece, font=(\"Helvetica\", 21),bg=color)\n",
    "    #             self.label.grid(row=i, column=j)\n",
    "    #     self.root.update()\n",
    "\n",
    "    def is_valid(self,move):\n",
    "        move=self.decode_move(move)\n",
    "        i,j=move\n",
    "        if(self.board[i,j]==\"\"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def close(self):\n",
    "        self.root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b58201-0d11-4a9c-8a58-f85ebcf827bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate,horizon,actor):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self.build_model()\n",
    "        self.horizon = horizon\n",
    "        self.model.apply(self.init_weights)  # Apply random weight initialization\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        # self.model.compile(optimizer=self.optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def build_model(self):\n",
    "        model=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=state_dim[0], out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * state_dim[1] * state_dim[2], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "            init.normal_(layer.weight, mean=0.0, std=0.05)\n",
    "            init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    \n",
    "    def get_actions_prob(self, states):\n",
    "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            action_probs_tensor = self.model(states_tensor)\n",
    "        action_probs = action_probs_tensor.numpy()\n",
    "        return action_probs\n",
    "\n",
    "    def train(self, states, actions, advantages,masks):\n",
    "        masks_tensor = torch.tensor(masks,dtype=torch.float32)\n",
    "        advantages_tensor:torch.Tensor = torch.tensor(advantages,dtype=torch.float32)\n",
    "        advantages_tensor = torch.squeeze(advantages_tensor)\n",
    "        actions_tensor = torch.tensor(actions,dtype=torch.float32)\n",
    "        states=np.array(states)\n",
    "        states_tensor = torch.tensor(states,dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            probs = self.model(states_tensor)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "        # entropy_loss = dist.entropy().mean()\n",
    "            o_log_probs = dist.log_prob(actions_tensor)\n",
    "        old_log_probs = torch.tensor(o_log_probs.numpy(),dtype=torch.float32) \n",
    "        n_batches = 2\n",
    "        horizon = int(self.horizon)\n",
    "        batch_size = horizon//n_batches\n",
    "        for epoch in range(2):\n",
    "            batch_starts = np.arange(0,horizon,batch_size)\n",
    "            indices = np.arange(horizon,dtype = np.int32)\n",
    "            np.random.shuffle(indices)\n",
    "            batches = [indices[i:i+batch_size] for i in batch_starts]\n",
    "            for batch in batches:\n",
    "                states_batch = states_tensor[batch]\n",
    "                old_log_probs_batch = old_log_probs[batch]\n",
    "                actions_batch = actions_tensor[batch]\n",
    "                masks_batch = masks_tensor[batch]\n",
    "                advantages_batch = advantages_tensor[batch]\n",
    "                probs = self.model(states_batch)\n",
    "                dist = torch.distributions.Categorical(probs=probs)\n",
    "                entropy = torch.mean(dist.entropy())\n",
    "                new_log_probs = dist.log_prob(actions_batch)\n",
    "                prob_ratio = torch.exp(new_log_probs-old_log_probs_batch)\n",
    "                weighted_ratio = prob_ratio * advantages_batch\n",
    "                clipped_prob_ratio = torch.clamp(prob_ratio,1-0.2,1+0.2)\n",
    "                weighted_clipped_ratio = advantages_batch * clipped_prob_ratio\n",
    "                loss = -torch.min(weighted_ratio,weighted_clipped_ratio).mean()\n",
    "                total_actor_loss = loss -0.4*entropy\n",
    "                self.optimizer.zero_grad()\n",
    "                total_actor_loss.backward()\n",
    "                clip_grad_value_(self.model.parameters(),10000)\n",
    "                clip_grad_norm_(self.model.parameters(),max_norm=0.5)\n",
    "                self.optimizer.step()\n",
    "        # actor_loss = -log_probs * advantages_tensor\n",
    "        # loss = actor_loss.mean() - 0.50 * entropy_loss\n",
    "        # self.optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # self.optimizer.step()\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, learning_rate,horizon,critic):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.build_model()\n",
    "        self.horizon = horizon\n",
    "        self.model.apply(self.init_weights)  # Apply random weight initialization\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        # self.model.compile(optimizer=self.optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    def build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=state_dim[0], out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * state_dim[1] * state_dim[2], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "            init.normal_(layer.weight, mean=0.0, std=0.05)\n",
    "            init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def get_values(self, states):\n",
    "        states_tensor = torch.tensor(np.array(states),dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            values_tensor = self.model(states_tensor)\n",
    "        values_tensor = torch.squeeze(values_tensor)\n",
    "        values = values_tensor.numpy()\n",
    "        return values\n",
    "\n",
    "    def train(self, states, discounted_rewards):\n",
    "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        discounted_rewards_tensor = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            old_values = self.model(states_tensor)\n",
    "        n_batches = 2\n",
    "        horizon = self.horizon\n",
    "        batch_size = horizon//n_batches\n",
    "        for epoch in range(2):\n",
    "            batch_starts = np.arange(0,horizon//n_batches)\n",
    "            indices = np.arange(horizon,dtype=np.int32)\n",
    "            np.random.shuffle(indices)\n",
    "            batches = [indices[i:i+batch_size] for i in batch_starts]\n",
    "            for batch in batches:\n",
    "                states_batch = states_tensor[batch]\n",
    "                discounted_rewards_batch = discounted_rewards_tensor[batch]\n",
    "                old_values_batch = old_values[batch]\n",
    "                new_values = self.model(states_batch)\n",
    "                clipped_values = old_values_batch + torch.clamp(new_values - old_values_batch,-0.2,+0.2)\n",
    "                loss1 = (discounted_rewards_batch - new_values)**2\n",
    "                loss2 = (discounted_rewards_batch - clipped_values)\n",
    "                loss = 0.5*torch.max(loss1,loss2).mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        values = self.model(states_tensor)\n",
    "        loss = torch.mean(0.5*(discounted_rewards_tensor - values)**2)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_value_(self.model.parameters(),10000)\n",
    "        clip_grad_norm_(self.model.parameters(),max_norm=0.5)\n",
    "        self.optimizer.step()\n",
    "        del states_tensor, discounted_rewards_tensor, values, loss\n",
    "\n",
    "# A2C algorithm\n",
    "class A2C:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate_actor, learning_rate_critic,horizon,actor,critic):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.horizon = horizon\n",
    "        self.actor = Actor(state_dim, action_dim, learning_rate_actor,horizon,actor)\n",
    "        self.critic = Critic(state_dim, learning_rate_critic,horizon,critic)\n",
    "        \n",
    "\n",
    "    def select_action(self, state,legal_actions,temp=0):\n",
    "        assert len(legal_actions) > 0\n",
    "        action_probs = self.actor.get_actions_prob(state)\n",
    "        debugged_state= np.swapaxes(state,-3,-1)\n",
    "        masks= np.zeros((self.action_dim,),dtype=np.float32)\n",
    "        masks[legal_actions] = 1\n",
    "        probs = action_probs[0]\n",
    "        if(temp==1):\n",
    "            print(probs)\n",
    "            probs = probs * masks\n",
    "            probs = probs / np.sum(probs,axis=-1)\n",
    "            action=np.argmax(probs)\n",
    "        else:\n",
    "            # print(probs)\n",
    "            action = np.random.choice(self.action_dim,p=probs)\n",
    "        return action\n",
    "\n",
    "    def train(self,states_batch,action_batch,returns_batch,masks):\n",
    "        # states_batch=[]\n",
    "        # action_batch=[]\n",
    "        # returns_batch=[]\n",
    "        # masks=[]\n",
    "        # # print(memory_batch)\n",
    "        # for i in range(len(memory_batch)):\n",
    "        #     states_batch.append(memory_batch[i][0])\n",
    "        #     action_batch.append(memory_batch[i][1])\n",
    "        #     returns_batch.append(memory_batch[i][2])\n",
    "        #     masks.append(memory_batch[i][3])\n",
    "        # states_batch=np.array(states_batch)\n",
    "        # action_batch=np.array(action_batch)\n",
    "        # returns_batch=np.array(returns_batch)\n",
    "        # masks=np.array(masks)\n",
    "        values = self.critic.get_values(states_batch)\n",
    "        advantages = returns_batch - values\n",
    "        self.actor.train(states_batch, action_batch, advantages,masks)\n",
    "        self.critic.train(states_batch, returns_batch)\n",
    "        return advantages\n",
    "        del values,next_values,td_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c21e24-7d93-4c2c-a973-aebdeca020ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_vs_random_player(a2c:A2C,n_games:int):\n",
    "    env  = TicTacToe()\n",
    "    endingscore = 0\n",
    "    for i in range(n_games):\n",
    "        agent_player = random.randint(0,1) %2\n",
    "        random_player = 1- agent_player\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        current_player = 0\n",
    "        while not done:\n",
    "            # print(env.board)\n",
    "            moves = env.GetPosMoves(env.board)\n",
    "            if current_player == agent_player:\n",
    "                action = a2c.select_action([state],moves)\n",
    "            else:\n",
    "                # action = int(input()) # for playing against a human\n",
    "                action = np.random.choice(moves)\n",
    "\n",
    "            next_state, reward , done = env.step(action)\n",
    "            current_player = 1-current_player\n",
    "            state= next_state\n",
    "\n",
    "        if current_player != agent_player:\n",
    "            if(reward<=-0.25):\n",
    "                endingscore -= -1\n",
    "\n",
    "    agentwins = endingscore\n",
    "    return agentwins / n_games * 100\n",
    "\n",
    "\n",
    "def calculate_returns(rewards,dones,last_value):\n",
    "    rewards = np.array(rewards)\n",
    "    returns = np.zeros_like(rewards)\n",
    "\n",
    "    gamma = 1.0\n",
    "    T = len(rewards)\n",
    "    flag = 0\n",
    "    next_value = last_value\n",
    "    for t in reversed(range(T)):\n",
    "        opponent_reward = rewards[t]\n",
    "        current_reward = -opponent_reward\n",
    "        if dones[t] and rewards[t]==1:\n",
    "            next_value = 0\n",
    "            flag = 1\n",
    "            temp=int(t)\n",
    "        elif dones[t] and rewards[t]!=-1:\n",
    "            next_value = 0\n",
    "            flag = 0\n",
    "        current_value = current_reward - gamma * next_value\n",
    "        if(flag and temp==t):\n",
    "            returns[t] = current_value\n",
    "            next_value = current_value\n",
    "            temp=-2\n",
    "        elif(flag and temp!=t):\n",
    "            returns[t] = 0.005\n",
    "            next_value = current_value\n",
    "        else:\n",
    "            returns[t] = current_value\n",
    "            next_value = current_value\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e57f4-6a23-4865-88f8-f9d0d767f5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 1586\n",
      "steps: 100 Win ratio against random player 8.00\n",
      "count: 1793\n",
      "steps: 200 Win ratio against random player 4.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_fn = lambda : TicTacToe()\n",
    "    env = env_fn()\n",
    "    # moded\n",
    "    state_dim= env.observation_space.shape\n",
    "    action_dim=9\n",
    "    learning_rate_actor = 0.0001\n",
    "    learning_rate_critic = 0.0001\n",
    "    horizon = 64\n",
    "    a2c = A2C(state_dim, action_dim, learning_rate_actor, learning_rate_critic,horizon,actor=None,critic=None)\n",
    "    count1=0\n",
    "    max_steps = 20000\n",
    "    total_steps=0\n",
    "    state=env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    all_masks = []\n",
    "    memory=[]\n",
    "    while total_steps < max_steps:\n",
    "        for step in range(horizon):\n",
    "            moves = env.GetPosMoves(env.board)\n",
    "            masks= np.zeros((action_dim,),dtype=np.float32)\n",
    "            masks[moves] = 1\n",
    "            # print(env.steps,moves)\n",
    "            temp1= a2c.select_action([state],moves)\n",
    "            is_legal_action=env.is_valid(temp1)\n",
    "            if(is_legal_action==False):\n",
    "                count1+=1\n",
    "            temp=int(temp1)\n",
    "            action = temp\n",
    "            next_state , reward , done = env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            all_masks.append(masks)\n",
    "            if done:\n",
    "                next_state = env.reset()\n",
    "            state = next_state\n",
    "        total_steps+=1\n",
    "        # print(total_steps)\n",
    "        last_state=state\n",
    "        # print(state)\n",
    "        last_value = a2c.critic.get_values([state])\n",
    "        # print(last_value)\n",
    "        returns = calculate_returns(rewards,dones,last_value)\n",
    "        # for i in range(len(states)):\n",
    "        #     memory.append([states[i],actions[i],returns[i],all_masks[i]])\n",
    "        # if(len(states)<batch_size):\n",
    "        #     pass\n",
    "        # else:\n",
    "        #     memory=random.sample(memory,batch_size)\n",
    "        \n",
    "        ad=a2c.train(\n",
    "                np.array(states),\n",
    "                np.array(actions),\n",
    "                np.array(returns),\n",
    "                np.array(all_masks)\n",
    "            )\n",
    "            \n",
    "        if(total_steps % 100==99):\n",
    "            print(\"count:\",count1)\n",
    "            print(\"steps:\",total_steps+1,end=\" \")\n",
    "            win_ratio = test_vs_random_player(a2c , 50)\n",
    "            print(f\"Win ratio against random player {win_ratio:0.2f}\")\n",
    "            count1=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eda2237-e0b1-44a1-8659-1b9ae7271f36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '' '']\n",
      " ['' '' '']\n",
      " ['' '' '']]\n",
      "[2.6911090e-08 6.6771291e-09 7.8482175e-05 9.9991953e-01 1.9276690e-06\n",
      " 7.4280870e-10 3.2360006e-09 9.3368534e-17 3.1466850e-17]\n",
      "[['' '' '']\n",
      " ['X' '' '']\n",
      " ['' '' '']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '' '']\n",
      " ['X' 'O' '']\n",
      " ['' '' '']]\n",
      "[4.4219507e-12 1.0566520e-10 9.9999571e-01 5.9772226e-22 1.4183807e-33\n",
      " 3.9399789e-11 4.3136674e-06 4.3865581e-24 5.9690835e-16]\n",
      "[['' '' 'X']\n",
      " ['X' 'O' '']\n",
      " ['' '' '']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '' 'X']\n",
      " ['X' 'O' '']\n",
      " ['' '' 'O']]\n",
      "[5.5009048e-02 9.4499081e-01 1.3876283e-19 1.9920178e-10 3.6901888e-25\n",
      " 7.6213652e-08 2.5072731e-13 5.0766733e-15 1.7916055e-34]\n",
      "[['' 'X' 'X']\n",
      " ['X' 'O' '']\n",
      " ['' '' 'O']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '' '']\n",
      " ['' '' '']\n",
      " ['' '' '']]\n",
      "[2.6911090e-08 6.6771291e-09 7.8482175e-05 9.9991953e-01 1.9276690e-06\n",
      " 7.4280870e-10 3.2360006e-09 9.3368534e-17 3.1466850e-17]\n",
      "[['' '' '']\n",
      " ['X' '' '']\n",
      " ['' '' '']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' '' '']\n",
      " ['X' '' '']\n",
      " ['' '' '']]\n",
      "[3.5119617e-21 2.1493612e-08 4.8478257e-09 2.6436589e-19 6.5619803e-05\n",
      " 7.4102671e-04 9.9919337e-01 1.3085020e-18 4.8949346e-16]\n",
      "[['O' '' '']\n",
      " ['X' '' '']\n",
      " ['X' '' '']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' '' '']\n",
      " ['X' '' '']\n",
      " ['X' '' 'O']]\n",
      "[1.3153297e-12 2.4482242e-07 6.1439894e-13 3.5134391e-15 5.6451043e-08\n",
      " 9.9999976e-01 2.9993248e-25 8.0584688e-14 1.5398738e-32]\n",
      "[['O' '' '']\n",
      " ['X' '' 'X']\n",
      " ['X' '' 'O']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    }
   ],
   "source": [
    "win_ratio = test_vs_random_player(a2c , 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
